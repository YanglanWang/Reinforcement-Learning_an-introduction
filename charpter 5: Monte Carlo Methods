
# it is
out first learning methods for estimating value functions and discovering optimal policies. At this point, we begin to quest the reinforcemnet learning without assuming complete knowledge of the environment.

To ensure that well-defined returns are available, we define Monte Carlo method only for episodic tasks.That is, we assume experience is divided into episodes, and all episodes eventually terminate no matter what actions are selected. It is only upon the completion of an episode that value estimates and policies are changed. Monte Carlo methods are thus incremental in an episode-by-episode sense, but not in a step-by-step sense.
here we use "Monte Carlo" specifically for methods based on averaging complete returns(as opposed to methods that learn from partial returns considered in the next charpter.)

5.1 Monte Carlo Policy Evaluation
The difference between DP diagram and Monte Carlo diagram reflects the fundamental difference between the algorithm. First, DP diagram shows only all possible transitions, the Monte Carlo diagram shows only those sampled on the one episode. Second, DP diagram includes only one-step transitions, the Monte Carlo diagram goes all the way to the end of the episode.
